{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb8991c-889d-4025-9246-592ef5df55d2",
   "metadata": {},
   "source": [
    "# Architecture \n",
    "It is a language model that utilizes a transformer based architecture and comprises of several key components like Input Embeddings, Encoder layers, Decoder layers and Output Layers.\n",
    "1. Input Embedding : In this the input text is converted to numerical representations that can be understood by the model. The embedding layer is being deployed for this task which maps each word or token in the input seq to a high dim vector.\n",
    "2. Encoder layer - GPT2 consists of multiple identical encoder layers stacked over each other. Each encoder layer has two sub layers which are a self attention mechanism and feed forwd network. The self attention mechanism allows the model to weigh the importance of diff words or tokens with inp. seq thereby capturing the dependencies and relationships betw. them. The feed forward network processes the self attn outputs to gen more complex representations.\n",
    "3. Decoder layer - It follows the encoder layers and has a similar structure as it also consists of self attention and feed forward layers. Just that in this the decoder layer is conditioned on the context from the prev. tokens enabling autoregressive generation. This means the model predicts the next word in the seq based on the context it has learned so far.\n",
    "4. Output layer - The final layer of GPT2 is a linear transformation followed by a softmax activation function. This layer produces the prob. distribution over the vocab for the next word in the sequence. It alows the model to generate text by sampling from the distribution or choosing the word with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ccf3d7-8dff-4ff7-8e37-aea9b4a0e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This snippet imports the necessary libraries and modules for the code. We import torch for PyTorch functionality, DataLoader for creating data loaders, GPT2LMHeadModel and GPT2Tokenizer from transformers for the\n",
    "#GPT-2 model and tokenizer, and AdamW for the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b32c0b7-515e-44f0-be03-d4a5d9dbfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader # from pytorch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW # from hugging face\n",
    "from datasets import load_dataset # from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3215c727-6388-4f02-adef-025291f55aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.22.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (23.2)\n",
      "Downloading plotly-5.22.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.22.0 tenacity-8.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac89c5f0-1361-4069-94c2-e8a4bc488af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1acb4706-2fe4-4d97-aa34-716bdd7c781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('/tf/tensorflow-tutorials/medium_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "755bd3d5-5891-4f07-b7ec-a26e8566c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataframe['text'][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e5bce48-3622-40d1-93dc-04cfc3e5ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0c68538-4616-41a8-917a-849c6e3b5dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0               Mental Note Vol. 24   \n",
       "1         Your Brain On Coronavirus   \n",
       "2                    Mind Your Nose   \n",
       "3          The 4 Purposes of Dreams   \n",
       "4  Surviving a Rod Through the Head   \n",
       "\n",
       "                                                text  \\\n",
       "0  Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1  Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2  Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3  Passionate about the synergy between science a...   \n",
       "4  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/invisible-illness/mental-no...            ['Ryan Fan']   \n",
       "1  https://medium.com/age-of-awareness/how-the-pa...       ['Simon Spichak']   \n",
       "2  https://medium.com/neodotlife/mind-your-nose-f...                      []   \n",
       "3  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "4  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-26 03:38:10.479000+00:00   \n",
       "1  2020-09-23 22:10:17.126000+00:00   \n",
       "2  2020-10-10 20:17:37.132000+00:00   \n",
       "3  2020-12-21 16:05:19.524000+00:00   \n",
       "4  2020-02-26 00:01:01.576000+00:00   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1  ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2  ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3  ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4  ['Brain', 'Health', 'Development', 'Psychology...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "847e352c-0f0d-4323-97af-ae86165efda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'url', 'authors', 'timestamp', 'tags'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec1e84ca-291c-4fa8-8f75-6eb9ecbca01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dataframe) == len(dataframe[\"url\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4a3f62c-b653-457a-b9ef-5dab1b50318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 192368 articles\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(dataframe['url'].unique())} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305af3b-788d-4972-a869-26e9be1d2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = [tag for tags_list in dataframe[\"tags\"] for tag in eval(tags_list)]\n",
    "d_tags_counter = Counter(all_tags)\n",
    "tags, frequencies = list(zip(*d_tags_counter.most_common(n=50)))\n",
    "\n",
    "fig = px.bar(x=tags, y=frequencies)\n",
    "fig.update_xaxes(title=\"tags\")\n",
    "fig.update_yaxes(title=\"frequencies\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4c1d9f0-5a6b-4329-9716-c019255fcac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192363</th>\n",
       "      <td>Why do you need a cleaning service?</td>\n",
       "      <td>What could be more important than having a tid...</td>\n",
       "      <td>https://medium.com/@ozneedcleaningau/why-do-yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-11-16 08:17:08.950000+00:00</td>\n",
       "      <td>['Cleaning', 'Cleaning Services', 'Cleaning Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>Daily cleaning and maintenance of bedding</td>\n",
       "      <td>Daily cleaning and maintenance of bedding\\n\\nW...</td>\n",
       "      <td>https://medium.com/@a198blwt/daily-cleaning-an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-11-16 05:27:05.359000+00:00</td>\n",
       "      <td>['Bedding', 'Cleaning', 'Maintain']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192365</th>\n",
       "      <td>Beneficial Advice on Bond Cleaning!</td>\n",
       "      <td>The most important chore at the end is bond cl...</td>\n",
       "      <td>https://medium.com/@princegohil/beneficial-adv...</td>\n",
       "      <td>['Prince Shrawan']</td>\n",
       "      <td>2021-11-26 08:20:27.660000+00:00</td>\n",
       "      <td>['Cleaning', 'End Of Lease Cleaning', 'Cleaners']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192366</th>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps</td>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps\\n\\nHey...</td>\n",
       "      <td>https://medium.com/@lifeinromania/how-i-learne...</td>\n",
       "      <td>['Sam Ursu']</td>\n",
       "      <td>2017-11-27 08:09:19.025000+00:00</td>\n",
       "      <td>['Romania', 'Language Learning', 'Storyofmylife']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192367</th>\n",
       "      <td>Trying Pimsleur Cantonese in Hong Kong</td>\n",
       "      <td>Over the past few years, I’ve heard a number o...</td>\n",
       "      <td>https://medium.com/toshuo/trying-pimsleur-cant...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-06-15 02:24:35.659000+00:00</td>\n",
       "      <td>['Hong Kong', 'Cantonese', 'Language Learning'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192368 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title  \\\n",
       "0                             Mental Note Vol. 24   \n",
       "1                       Your Brain On Coronavirus   \n",
       "2                                  Mind Your Nose   \n",
       "3                        The 4 Purposes of Dreams   \n",
       "4                Surviving a Rod Through the Head   \n",
       "...                                           ...   \n",
       "192363        Why do you need a cleaning service?   \n",
       "192364  Daily cleaning and maintenance of bedding   \n",
       "192365        Beneficial Advice on Bond Cleaning!   \n",
       "192366    How I Learned Romanian in 37 Easy Steps   \n",
       "192367     Trying Pimsleur Cantonese in Hong Kong   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1       Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2       Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3       Passionate about the synergy between science a...   \n",
       "4       You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "...                                                   ...   \n",
       "192363  What could be more important than having a tid...   \n",
       "192364  Daily cleaning and maintenance of bedding\\n\\nW...   \n",
       "192365  The most important chore at the end is bond cl...   \n",
       "192366  How I Learned Romanian in 37 Easy Steps\\n\\nHey...   \n",
       "192367  Over the past few years, I’ve heard a number o...   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://medium.com/invisible-illness/mental-no...   \n",
       "1       https://medium.com/age-of-awareness/how-the-pa...   \n",
       "2       https://medium.com/neodotlife/mind-your-nose-f...   \n",
       "3       https://medium.com/science-for-real/the-4-purp...   \n",
       "4       https://medium.com/live-your-life-on-purpose/s...   \n",
       "...                                                   ...   \n",
       "192363  https://medium.com/@ozneedcleaningau/why-do-yo...   \n",
       "192364  https://medium.com/@a198blwt/daily-cleaning-an...   \n",
       "192365  https://medium.com/@princegohil/beneficial-adv...   \n",
       "192366  https://medium.com/@lifeinromania/how-i-learne...   \n",
       "192367  https://medium.com/toshuo/trying-pimsleur-cant...   \n",
       "\n",
       "                       authors                         timestamp  \\\n",
       "0                 ['Ryan Fan']  2020-12-26 03:38:10.479000+00:00   \n",
       "1            ['Simon Spichak']  2020-09-23 22:10:17.126000+00:00   \n",
       "2                           []  2020-10-10 20:17:37.132000+00:00   \n",
       "3       ['Eshan Samaranayake']  2020-12-21 16:05:19.524000+00:00   \n",
       "4             ['Rishav Sinha']  2020-02-26 00:01:01.576000+00:00   \n",
       "...                        ...                               ...   \n",
       "192363                      []  2021-11-16 08:17:08.950000+00:00   \n",
       "192364                      []  2021-11-16 05:27:05.359000+00:00   \n",
       "192365      ['Prince Shrawan']  2021-11-26 08:20:27.660000+00:00   \n",
       "192366            ['Sam Ursu']  2017-11-27 08:09:19.025000+00:00   \n",
       "192367                      []  2017-06-15 02:24:35.659000+00:00   \n",
       "\n",
       "                                                     tags  \n",
       "0       ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1       ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2       ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3       ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4       ['Brain', 'Health', 'Development', 'Psychology...  \n",
       "...                                                   ...  \n",
       "192363  ['Cleaning', 'Cleaning Services', 'Cleaning Co...  \n",
       "192364                ['Bedding', 'Cleaning', 'Maintain']  \n",
       "192365  ['Cleaning', 'End Of Lease Cleaning', 'Cleaners']  \n",
       "192366  ['Romania', 'Language Learning', 'Storyofmylife']  \n",
       "192367  ['Hong Kong', 'Cantonese', 'Language Learning'...  \n",
       "\n",
       "[192368 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cabc3443-041f-41da-9a4e-b381ae49fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f240bdcf-ebf0-48dd-a130-368d335e1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f75dbac-5764-42cf-9f0b-f123806fa540",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataframe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f6b4075-ac52-4b62-b51d-53d5acfa8671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153894, 38474)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140312d9-f696-40b0-a1d8-791f5ebed049",
   "metadata": {},
   "source": [
    "# GPT2 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "699c9d24-ae17-4f2b-a13a-169ce31ddcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ab08482324124be3f521081672de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  12%|#1        | 178M/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383e2e35661c4ffd82c276548f869056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cfc7eb4c2d424481f1440cac972a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e1d0775be4862b7e76125d8f96693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97c5ab1880d45e6ba247d12078e0993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3831c04d4ed445888758aa5fe04e8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8613a1-dcba-4619-b988-e7e34129030d",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a832fc3-807b-4fe7-be5c-5aa78197aa7a",
   "metadata": {},
   "source": [
    "# ANY of these models can be used for Effective text generation, for gpu memory limitation, i used distil gpt2 version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d559a-7b54-426b-ada2-767897015970",
   "metadata": {},
   "source": [
    "# DISTIL GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6c82f-8156-4c87-b8c7-3ed4d4a629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1df27fa8-6b00-4ad2-af88-fc7b8d804d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_dataset(text):\n",
    "    return tokenizer(text, truncation=True, max_length=512, padding=\"max_length\",return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e745df1d-2b12-4f07-b304-c6fb5e397ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokenized_text']= train_data['text'].apply(tokenizer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9095d4d2-12e9-4a6e-8354-7537d5719200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Final_Dataset_Short.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a6b5035-d0ea-4055-a1b5-669708c796de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are loading saved csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc6450f8-9afe-40e2-9f23-21b94e009872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7935/1069282606.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('/tf/Final_Dataset_Short.csv')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('/tf/Final_Dataset_Short.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4ea2e12-694f-4c4a-9b5d-3d5c3d9753b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [input_ids, attention_mask]\n",
       "1         [input_ids, attention_mask]\n",
       "2         [input_ids, attention_mask]\n",
       "3         [input_ids, attention_mask]\n",
       "4         [input_ids, attention_mask]\n",
       "                     ...             \n",
       "153889    [input_ids, attention_mask]\n",
       "153890    [input_ids, attention_mask]\n",
       "153891    [input_ids, attention_mask]\n",
       "153892    [input_ids, attention_mask]\n",
       "153893    [input_ids, attention_mask]\n",
       "Name: tokenized_text, Length: 153894, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"tokenized_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8757c96c-41af-4531-926d-5ef30bfb627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             object\n",
       "text              object\n",
       "url               object\n",
       "authors           object\n",
       "timestamp         object\n",
       "tags              object\n",
       "tokenized_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = train_data.dtypes\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22864c19-52e6-4a62-a7ad-8104a3556a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How To Make A Man Obsessed With You | What Men...</td>\n",
       "      <td>How To Make A Man Obsessed With You | What Men...</td>\n",
       "      <td>https://medium.com/@moikabu43/how-to-make-a-ma...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-09-15 18:14:06.621000+00:00</td>\n",
       "      <td>['Relationships Love Dating', 'Relationship Ad...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop using Pandas and start using Spark with S...</td>\n",
       "      <td>Data transformations\\n\\nMost (if not all) of t...</td>\n",
       "      <td>https://towardsdatascience.com/stop-using-pand...</td>\n",
       "      <td>['Chloe Connor']</td>\n",
       "      <td>2020-06-07 18:56:07.675000+00:00</td>\n",
       "      <td>['Scala', 'Spark', 'Towards Data Science', 'Pa...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entanglements</td>\n",
       "      <td>I tell stories entangled in the ridges of my m...</td>\n",
       "      <td>https://medium.com/scribe/entanglements-108bad...</td>\n",
       "      <td>['Uchechi Obasi']</td>\n",
       "      <td>2020-08-06 12:25:00.903000+00:00</td>\n",
       "      <td>['Poetry', 'Haiku', 'Heartbreak', 'Dating', 'L...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SpanFact: Fix your factually incorrect summaries</td>\n",
       "      <td>SpanFact: Fix your factually incorrect summari...</td>\n",
       "      <td>https://towardsdatascience.com/spanfact-fix-yo...</td>\n",
       "      <td>['Rohit Pillai']</td>\n",
       "      <td>2020-12-17 05:06:47.959000+00:00</td>\n",
       "      <td>['Naturallanguageprocessing', 'Summarization',...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVM Android Integration is live for Aion Network</td>\n",
       "      <td>The Pocket team loves celebrating with its pee...</td>\n",
       "      <td>https://medium.com/pocket-network/avm-android-...</td>\n",
       "      <td>['Pocket Network']</td>\n",
       "      <td>2019-08-06 16:12:04.050000+00:00</td>\n",
       "      <td>['Blockchain', 'Avm', 'Smart Contracts', 'Aion...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  How To Make A Man Obsessed With You | What Men...   \n",
       "1  Stop using Pandas and start using Spark with S...   \n",
       "2                                      Entanglements   \n",
       "3   SpanFact: Fix your factually incorrect summaries   \n",
       "4   AVM Android Integration is live for Aion Network   \n",
       "\n",
       "                                                text  \\\n",
       "0  How To Make A Man Obsessed With You | What Men...   \n",
       "1  Data transformations\\n\\nMost (if not all) of t...   \n",
       "2  I tell stories entangled in the ridges of my m...   \n",
       "3  SpanFact: Fix your factually incorrect summari...   \n",
       "4  The Pocket team loves celebrating with its pee...   \n",
       "\n",
       "                                                 url             authors  \\\n",
       "0  https://medium.com/@moikabu43/how-to-make-a-ma...                  []   \n",
       "1  https://towardsdatascience.com/stop-using-pand...    ['Chloe Connor']   \n",
       "2  https://medium.com/scribe/entanglements-108bad...   ['Uchechi Obasi']   \n",
       "3  https://towardsdatascience.com/spanfact-fix-yo...    ['Rohit Pillai']   \n",
       "4  https://medium.com/pocket-network/avm-android-...  ['Pocket Network']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2021-09-15 18:14:06.621000+00:00   \n",
       "1  2020-06-07 18:56:07.675000+00:00   \n",
       "2  2020-08-06 12:25:00.903000+00:00   \n",
       "3  2020-12-17 05:06:47.959000+00:00   \n",
       "4  2019-08-06 16:12:04.050000+00:00   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Relationships Love Dating', 'Relationship Ad...   \n",
       "1  ['Scala', 'Spark', 'Towards Data Science', 'Pa...   \n",
       "2  ['Poetry', 'Haiku', 'Heartbreak', 'Dating', 'L...   \n",
       "3  ['Naturallanguageprocessing', 'Summarization',...   \n",
       "4  ['Blockchain', 'Avm', 'Smart Contracts', 'Aion...   \n",
       "\n",
       "                tokenized_text  \n",
       "0  [input_ids, attention_mask]  \n",
       "1  [input_ids, attention_mask]  \n",
       "2  [input_ids, attention_mask]  \n",
       "3  [input_ids, attention_mask]  \n",
       "4  [input_ids, attention_mask]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "515b8389-5326-4ade-84ea-ac7a3ba18ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45be2b-2b4f-4f48-8c8a-c4340fa4d5e4",
   "metadata": {},
   "source": [
    "# extract inputs ids, attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "da8b395c-d380-4aad-beda-964a664f04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ast\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "     \n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        tokenized_text = row['tokenized_text']\n",
    "        \n",
    "        # Check and print the type to understand what we are dealing with\n",
    "        #print(\"ROW : \", row)\n",
    "        #print(\"Type of tokenized_text:\", type(tokenized_text))\n",
    "        #print(\"Content of tokenized_text:\", tokenized_text)\n",
    "        \n",
    "        # Assuming 'tokenized_text' is a dictionary containing tensors for 'input_ids' and 'attention_mask'\n",
    "        \n",
    "        input_ids = tokenized_text['input_ids'].squeeze(0)  # Remove batch dimension if present\n",
    "        attention_mask = tokenized_text['attention_mask'].squeeze(0)  # Remove batch dimension if present\n",
    "        labels = input_ids.clone()  # Typically labels for a language model are the input_ids themselves\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ee0c0-4228-4ebe-9b07-1c36716c6a8c",
   "metadata": {},
   "source": [
    "# call dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "195efbb8-6d2f-4832-a123-f9a4545bc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataset\n",
    "dataset = TextDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "28beae88-45e8-476e-afc9-a8c4434002c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a83d2487-ec8a-4b98-acc9-b560cacc1106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe778c0b110>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12dca0-dabe-44b9-ad86-ad1c7321ac12",
   "metadata": {},
   "source": [
    "# check dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3656b93b-9988-4ba4-98da-164ceb40331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch fetched successfully!\n",
      "Input IDs: tensor([[ 1890,  1374,  5882,  ..., 50256, 50256, 50256],\n",
      "        [25681,    12,   265,  ...,  1738,   373,   780],\n",
      "        [ 1212,   318,   262,  ..., 23292,   306,   503],\n",
      "        ...,\n",
      "        [ 3844, 20544,   612,  ...,  3038,   318,  2045],\n",
      "        [   40,   892,   314,  ...,  7062,   284, 12383],\n",
      "        [   32,  1285,   550,  ...,  8033,   287,   290]])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Labels: tensor([[ 1890,  1374,  5882,  ..., 50256, 50256, 50256],\n",
      "        [25681,    12,   265,  ...,  1738,   373,   780],\n",
      "        [ 1212,   318,   262,  ..., 23292,   306,   503],\n",
      "        ...,\n",
      "        [ 3844, 20544,   612,  ...,  3038,   318,  2045],\n",
      "        [   40,   892,   314,  ...,  7062,   284, 12383],\n",
      "        [   32,  1285,   550,  ...,  8033,   287,   290]])\n"
     ]
    }
   ],
   "source": [
    "# Test fetching a batch\n",
    "try:\n",
    "    batch = next(iter(train_dataloader))\n",
    "    print(\"Batch fetched successfully!\")\n",
    "    print(\"Input IDs:\", batch['input_ids'])\n",
    "    print(\"Attention Mask:\", batch['attention_mask'])\n",
    "    print(\"Labels:\", batch['labels'])\n",
    "except Exception as e:\n",
    "    print(\"Failed to fetch a batch:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8ce50d-1027-4fad-87c5-d4e34507c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4933ea46-59ee-4e4a-9c19-fbff111439bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968d807-942d-46e7-838c-dbd3065511ad",
   "metadata": {},
   "source": [
    "# device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1052d01b-3bb4-4b92-bc4a-795e174e513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5087745-8a00-4f0b-ba1c-4e5eb71dae59",
   "metadata": {},
   "source": [
    "# “automatic mixed precision training”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7471ae49-4317-4fc6-8485-65482ac1b6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-0,Loss-5.136580467224121\n",
      "Step-1,Loss-3.4252097606658936\n",
      "Step-2,Loss-3.0653090476989746\n",
      "Step-3,Loss-3.4548635482788086\n",
      "Step-4,Loss-2.964229106903076\n",
      "Step-5,Loss-3.7356607913970947\n",
      "Step-6,Loss-4.038450241088867\n",
      "Step-7,Loss-3.439716100692749\n",
      "Step-8,Loss-2.9599533081054688\n",
      "Step-9,Loss-3.1705498695373535\n",
      "Step-10,Loss-3.3257651329040527\n",
      "Step-11,Loss-2.8328394889831543\n",
      "Step-12,Loss-5.30260705947876\n",
      "Step-13,Loss-3.334078550338745\n",
      "Step-14,Loss-2.7600533962249756\n",
      "Step-15,Loss-5.441909313201904\n",
      "Step-16,Loss-4.976123809814453\n",
      "Step-17,Loss-3.904688596725464\n",
      "Step-18,Loss-6.645779609680176\n",
      "Step-19,Loss-4.560384750366211\n",
      "Step-20,Loss-4.828579425811768\n",
      "Step-21,Loss-2.9035515785217285\n",
      "Step-22,Loss-3.716073989868164\n",
      "Step-23,Loss-2.8754332065582275\n",
      "Step-24,Loss-3.516566753387451\n",
      "Step-25,Loss-3.2154414653778076\n",
      "Step-26,Loss-3.1334550380706787\n",
      "Step-27,Loss-2.9585983753204346\n",
      "Step-28,Loss-2.839010715484619\n",
      "Step-29,Loss-3.506218910217285\n",
      "Step-30,Loss-3.3266377449035645\n",
      "Step-31,Loss-2.619534492492676\n",
      "Step-32,Loss-2.8677234649658203\n",
      "Step-33,Loss-3.002593994140625\n",
      "Step-34,Loss-2.5991110801696777\n",
      "Step-35,Loss-3.427055597305298\n",
      "Step-36,Loss-3.042037010192871\n",
      "Step-37,Loss-2.8590118885040283\n",
      "Step-38,Loss-2.7598917484283447\n",
      "Step-39,Loss-3.0844922065734863\n",
      "Step-40,Loss-3.2077112197875977\n",
      "Step-41,Loss-2.9343531131744385\n",
      "Step-42,Loss-3.302657127380371\n",
      "Step-43,Loss-2.9350056648254395\n",
      "Step-44,Loss-3.1782846450805664\n",
      "Step-45,Loss-2.6955034732818604\n",
      "Step-46,Loss-2.728841543197632\n",
      "Step-47,Loss-2.955349922180176\n",
      "Step-48,Loss-2.9801688194274902\n",
      "Step-49,Loss-2.762019157409668\n",
      "Step-50,Loss-2.860136032104492\n",
      "Step-51,Loss-3.4760921001434326\n",
      "Step-52,Loss-2.8934309482574463\n",
      "Step-53,Loss-3.054473400115967\n",
      "Step-54,Loss-1.7514225244522095\n",
      "Step-55,Loss-2.923863172531128\n",
      "Step-56,Loss-2.8357040882110596\n",
      "Step-57,Loss-3.0646679401397705\n",
      "Step-58,Loss-2.449167490005493\n",
      "Step-59,Loss-2.0296003818511963\n",
      "Step-60,Loss-1.998337745666504\n",
      "Step-61,Loss-2.650928020477295\n",
      "Step-62,Loss-2.8123090267181396\n",
      "Step-63,Loss-3.0525271892547607\n",
      "Step-64,Loss-2.5125033855438232\n",
      "Step-65,Loss-2.91477108001709\n",
      "Step-66,Loss-1.505049705505371\n",
      "Step-67,Loss-1.9946887493133545\n",
      "Step-68,Loss-3.3837413787841797\n",
      "Step-69,Loss-3.312384605407715\n",
      "Step-70,Loss-1.9268614053726196\n",
      "Step-71,Loss-0.5669880509376526\n",
      "Step-72,Loss-2.749462604522705\n",
      "Step-73,Loss-3.376159429550171\n",
      "Step-74,Loss-2.5452239513397217\n",
      "Step-75,Loss-2.8494293689727783\n",
      "Step-76,Loss-3.252145528793335\n",
      "Step-77,Loss-2.914689064025879\n",
      "Step-78,Loss-2.238069534301758\n",
      "Step-79,Loss-2.394111394882202\n",
      "Step-80,Loss-3.0951550006866455\n",
      "Step-81,Loss-3.5781571865081787\n",
      "Step-82,Loss-2.4612314701080322\n",
      "Step-83,Loss-2.1038520336151123\n",
      "Step-84,Loss-1.8421107530593872\n",
      "Step-85,Loss-1.9715518951416016\n",
      "Step-86,Loss-2.7029380798339844\n",
      "Step-87,Loss-2.767206907272339\n",
      "Step-88,Loss-3.19565749168396\n",
      "Step-89,Loss-2.24075984954834\n",
      "Step-90,Loss-3.2605092525482178\n",
      "Step-91,Loss-2.0254127979278564\n",
      "Step-92,Loss-2.0490360260009766\n",
      "Step-93,Loss-3.1385979652404785\n",
      "Step-94,Loss-1.6893188953399658\n",
      "Step-95,Loss-1.8976978063583374\n",
      "Step-96,Loss-2.24930477142334\n",
      "Step-97,Loss-3.1364853382110596\n",
      "Step-98,Loss-3.0498523712158203\n",
      "Step-99,Loss-1.5811731815338135\n",
      "Step-100,Loss-1.8190165758132935\n",
      "Step-101,Loss-1.8235958814620972\n",
      "Step-102,Loss-1.4702996015548706\n",
      "Step-103,Loss-2.879356622695923\n",
      "Step-104,Loss-2.4918229579925537\n",
      "Step-105,Loss-2.411545515060425\n",
      "Step-106,Loss-3.0714094638824463\n",
      "Step-107,Loss-3.3062736988067627\n",
      "Step-108,Loss-2.7795369625091553\n",
      "Step-109,Loss-2.5736730098724365\n",
      "Step-110,Loss-2.4632997512817383\n",
      "Step-111,Loss-2.8463377952575684\n",
      "Step-112,Loss-2.317732810974121\n",
      "Step-113,Loss-3.022102117538452\n",
      "Step-114,Loss-0.7229847311973572\n",
      "Step-115,Loss-3.165804147720337\n",
      "Step-116,Loss-3.0978071689605713\n",
      "Step-117,Loss-3.303008556365967\n",
      "Step-118,Loss-1.8458997011184692\n",
      "Step-119,Loss-2.499812602996826\n",
      "Step-120,Loss-3.2512247562408447\n",
      "Step-121,Loss-3.0963943004608154\n",
      "Step-122,Loss-3.2079334259033203\n",
      "Step-123,Loss-3.2491533756256104\n",
      "Step-124,Loss-1.6484105587005615\n",
      "Step-125,Loss-2.891433000564575\n",
      "Step-126,Loss-2.177380323410034\n",
      "Step-127,Loss-3.3964154720306396\n",
      "Step-128,Loss-2.3974931240081787\n",
      "Step-129,Loss-3.217052459716797\n",
      "Step-130,Loss-3.4058382511138916\n",
      "Step-131,Loss-2.6566720008850098\n",
      "Step-132,Loss-2.2849860191345215\n",
      "Step-133,Loss-3.0584447383880615\n",
      "Step-134,Loss-3.0587809085845947\n",
      "Step-135,Loss-2.3986780643463135\n",
      "Step-136,Loss-2.731945514678955\n",
      "Step-137,Loss-2.6462340354919434\n",
      "Step-138,Loss-2.1740713119506836\n",
      "Step-139,Loss-3.2103681564331055\n",
      "Step-140,Loss-2.86561918258667\n",
      "Step-141,Loss-1.7660276889801025\n",
      "Step-142,Loss-3.207760810852051\n",
      "Step-143,Loss-3.2609050273895264\n",
      "Step-144,Loss-2.790174961090088\n",
      "Step-145,Loss-2.974011182785034\n",
      "Step-146,Loss-3.2244012355804443\n",
      "Step-147,Loss-2.7939047813415527\n",
      "Step-148,Loss-1.8229905366897583\n",
      "Step-149,Loss-3.450083017349243\n",
      "Step-150,Loss-1.7591800689697266\n",
      "Step-151,Loss-1.852493166923523\n",
      "Step-152,Loss-3.0130224227905273\n",
      "Step-153,Loss-2.593796730041504\n",
      "Step-154,Loss-1.9418237209320068\n",
      "Step-155,Loss-2.5541493892669678\n",
      "Step-156,Loss-2.927220582962036\n",
      "Step-157,Loss-2.3929944038391113\n",
      "Step-158,Loss-1.6690070629119873\n",
      "Step-159,Loss-3.332834482192993\n",
      "Step-160,Loss-3.198209285736084\n",
      "Step-161,Loss-3.2103383541107178\n",
      "Step-162,Loss-2.4919068813323975\n",
      "Step-163,Loss-2.576228618621826\n",
      "Step-164,Loss-3.2395877838134766\n",
      "Step-165,Loss-1.8659697771072388\n",
      "Step-166,Loss-3.464334487915039\n",
      "Step-167,Loss-3.1843268871307373\n",
      "Step-168,Loss-3.274088144302368\n",
      "Step-169,Loss-3.0677735805511475\n",
      "Step-170,Loss-1.7737398147583008\n",
      "Step-171,Loss-2.352036237716675\n",
      "Step-172,Loss-2.88712215423584\n",
      "Step-173,Loss-3.0696394443511963\n",
      "Step-174,Loss-1.5827699899673462\n",
      "Step-175,Loss-2.81046986579895\n",
      "Step-176,Loss-2.925233840942383\n",
      "Step-177,Loss-3.00801682472229\n",
      "Step-178,Loss-1.570718765258789\n",
      "Step-179,Loss-1.2509115934371948\n",
      "Step-180,Loss-3.0952844619750977\n",
      "Step-181,Loss-2.995893716812134\n",
      "Step-182,Loss-2.049618721008301\n",
      "Step-183,Loss-2.1481099128723145\n",
      "Step-184,Loss-2.0643606185913086\n",
      "Step-185,Loss-1.5125577449798584\n",
      "Step-186,Loss-2.542853832244873\n",
      "Step-187,Loss-2.8516952991485596\n",
      "Step-188,Loss-2.9167466163635254\n",
      "Step-189,Loss-1.7735066413879395\n",
      "Step-190,Loss-1.5431909561157227\n",
      "Step-191,Loss-2.1076200008392334\n",
      "Step-192,Loss-3.400908946990967\n",
      "Step-193,Loss-2.8938210010528564\n",
      "Step-194,Loss-2.649522542953491\n",
      "Step-195,Loss-3.0920753479003906\n",
      "Step-196,Loss-2.2354860305786133\n",
      "Step-197,Loss-1.6160334348678589\n",
      "Step-198,Loss-2.4748997688293457\n",
      "Step-199,Loss-1.6193886995315552\n",
      "Step-200,Loss-2.9230172634124756\n",
      "Step-201,Loss-1.669966459274292\n",
      "Step-202,Loss-1.7302061319351196\n",
      "Step-203,Loss-2.8921940326690674\n",
      "Step-204,Loss-2.9991955757141113\n",
      "Step-205,Loss-2.6404247283935547\n",
      "Step-206,Loss-2.602337598800659\n",
      "Step-207,Loss-2.8128345012664795\n",
      "Step-208,Loss-2.7333273887634277\n",
      "Step-209,Loss-3.076756238937378\n",
      "Step-210,Loss-2.9041638374328613\n",
      "Step-211,Loss-2.5648460388183594\n",
      "Step-212,Loss-2.310464382171631\n",
      "Step-213,Loss-2.4529366493225098\n",
      "Step-214,Loss-2.5115065574645996\n",
      "Step-215,Loss-2.4587416648864746\n",
      "Step-216,Loss-2.704420328140259\n",
      "Step-217,Loss-1.7919015884399414\n",
      "Step-218,Loss-1.83465576171875\n",
      "Step-219,Loss-2.9588732719421387\n",
      "Step-220,Loss-2.2143943309783936\n",
      "Step-221,Loss-1.7614350318908691\n",
      "Step-222,Loss-2.8201355934143066\n",
      "Step-223,Loss-2.5829262733459473\n",
      "Step-224,Loss-2.007582902908325\n",
      "Step-225,Loss-3.2667222023010254\n",
      "Step-226,Loss-2.3067917823791504\n",
      "Step-227,Loss-3.134464740753174\n",
      "Step-228,Loss-2.1599905490875244\n",
      "Step-229,Loss-2.9730567932128906\n",
      "Step-230,Loss-1.788417935371399\n",
      "Step-231,Loss-2.951296091079712\n",
      "Step-232,Loss-2.6362390518188477\n",
      "Step-233,Loss-3.2430083751678467\n",
      "Step-234,Loss-2.907721996307373\n",
      "Step-235,Loss-3.1208126544952393\n",
      "Step-236,Loss-1.7659623622894287\n",
      "Step-237,Loss-2.7787342071533203\n",
      "Step-238,Loss-2.579584836959839\n",
      "Step-239,Loss-3.1050479412078857\n",
      "Step-240,Loss-1.9738969802856445\n",
      "Step-241,Loss-1.723469614982605\n",
      "Step-242,Loss-3.1436314582824707\n",
      "Step-243,Loss-2.813326835632324\n",
      "Step-244,Loss-3.0640316009521484\n",
      "Step-245,Loss-2.762856960296631\n",
      "Step-246,Loss-2.725390911102295\n",
      "Step-247,Loss-2.8287742137908936\n",
      "Step-248,Loss-3.28804874420166\n",
      "Step-249,Loss-2.844871997833252\n",
      "Step-250,Loss-0.8912820816040039\n",
      "Step-251,Loss-2.303832530975342\n",
      "Step-252,Loss-2.452233076095581\n",
      "Step-253,Loss-2.747013568878174\n",
      "Step-254,Loss-2.8928842544555664\n",
      "Step-255,Loss-2.389846086502075\n",
      "Step-256,Loss-1.2270677089691162\n",
      "Step-257,Loss-2.524108648300171\n",
      "Step-258,Loss-1.4581046104431152\n",
      "Step-259,Loss-3.017101287841797\n",
      "Step-260,Loss-2.1493144035339355\n",
      "Step-261,Loss-2.7160966396331787\n",
      "Step-262,Loss-2.7359306812286377\n",
      "Step-263,Loss-1.7643333673477173\n",
      "Step-264,Loss-1.8255153894424438\n",
      "Step-265,Loss-3.3818798065185547\n",
      "Step-266,Loss-2.566340923309326\n",
      "Step-267,Loss-3.04520583152771\n",
      "Step-268,Loss-3.296086311340332\n",
      "Step-269,Loss-1.7809466123580933\n",
      "Step-270,Loss-2.6841225624084473\n",
      "Step-271,Loss-3.0763721466064453\n",
      "Step-272,Loss-3.1282176971435547\n",
      "Step-273,Loss-1.1396551132202148\n",
      "Step-274,Loss-2.705409526824951\n",
      "Step-275,Loss-2.7887840270996094\n",
      "Step-276,Loss-2.8372342586517334\n",
      "Step-277,Loss-1.6740998029708862\n",
      "Step-278,Loss-1.7399476766586304\n",
      "Step-279,Loss-2.896207332611084\n",
      "Step-280,Loss-2.6323909759521484\n",
      "Step-281,Loss-3.2214794158935547\n",
      "Step-282,Loss-1.1041778326034546\n",
      "Step-283,Loss-2.921485424041748\n",
      "Step-284,Loss-2.71989107131958\n",
      "Step-285,Loss-3.578380584716797\n",
      "Step-286,Loss-2.4348244667053223\n",
      "Step-287,Loss-2.2233197689056396\n",
      "Step-288,Loss-2.8841733932495117\n",
      "Step-289,Loss-2.8151326179504395\n",
      "Step-290,Loss-2.0134291648864746\n",
      "Step-291,Loss-2.816587209701538\n",
      "Step-292,Loss-2.4320883750915527\n",
      "Step-293,Loss-1.8171180486679077\n",
      "Step-294,Loss-2.5829062461853027\n",
      "Step-295,Loss-2.7864692211151123\n",
      "Step-296,Loss-1.5721906423568726\n",
      "Step-297,Loss-2.0361247062683105\n",
      "Step-298,Loss-3.05424427986145\n",
      "Step-299,Loss-1.9649215936660767\n",
      "Step-300,Loss-1.6003787517547607\n",
      "Step-301,Loss-2.006669759750366\n",
      "Step-302,Loss-2.7061643600463867\n",
      "Step-303,Loss-2.378060817718506\n",
      "Step-304,Loss-2.770448923110962\n",
      "Step-305,Loss-2.075845956802368\n",
      "Step-306,Loss-3.078326463699341\n",
      "Step-307,Loss-2.25235915184021\n",
      "Step-308,Loss-2.932796001434326\n",
      "Step-309,Loss-1.6561979055404663\n",
      "Step-310,Loss-2.782134771347046\n",
      "Step-311,Loss-3.2004597187042236\n",
      "Step-312,Loss-3.302225112915039\n",
      "Step-313,Loss-2.4965171813964844\n",
      "Step-314,Loss-3.0846288204193115\n",
      "Step-315,Loss-2.86548113822937\n",
      "Step-316,Loss-0.6003906726837158\n",
      "Step-317,Loss-3.3713176250457764\n",
      "Step-318,Loss-2.699536085128784\n",
      "Step-319,Loss-3.0315048694610596\n",
      "Step-320,Loss-2.2736847400665283\n",
      "Step-321,Loss-2.6897029876708984\n",
      "Step-322,Loss-2.7475807666778564\n",
      "Step-323,Loss-2.9848344326019287\n",
      "Step-324,Loss-3.0091240406036377\n",
      "Step-325,Loss-3.1823410987854004\n",
      "Step-326,Loss-3.327162504196167\n",
      "Step-327,Loss-2.8255274295806885\n",
      "Step-328,Loss-2.1526377201080322\n",
      "Step-329,Loss-2.3734376430511475\n",
      "Step-330,Loss-2.636934518814087\n",
      "Step-331,Loss-2.745922088623047\n",
      "Step-332,Loss-1.8675616979599\n",
      "Step-333,Loss-2.456476926803589\n",
      "Step-334,Loss-2.8640737533569336\n",
      "Step-335,Loss-2.0040175914764404\n",
      "Step-336,Loss-2.567261219024658\n",
      "Step-337,Loss-2.913058280944824\n",
      "Step-338,Loss-1.3474513292312622\n",
      "Step-339,Loss-2.6896471977233887\n",
      "Step-340,Loss-1.8481518030166626\n",
      "Step-341,Loss-2.92742657661438\n",
      "Step-342,Loss-3.4143683910369873\n",
      "Step-343,Loss-2.5052578449249268\n",
      "Step-344,Loss-3.2757585048675537\n",
      "Step-345,Loss-3.019001007080078\n",
      "Step-346,Loss-2.316699981689453\n",
      "Step-347,Loss-2.02632474899292\n",
      "Step-348,Loss-2.9414939880371094\n",
      "Step-349,Loss-2.651233434677124\n",
      "Step-350,Loss-2.8305814266204834\n",
      "Step-351,Loss-2.4595916271209717\n",
      "Step-352,Loss-3.0912587642669678\n",
      "Step-353,Loss-2.138982057571411\n",
      "Step-354,Loss-3.0582797527313232\n",
      "Step-355,Loss-2.6037447452545166\n",
      "Step-356,Loss-2.6048333644866943\n",
      "Step-357,Loss-3.1908116340637207\n",
      "Step-358,Loss-3.010547161102295\n",
      "Step-359,Loss-1.8284015655517578\n",
      "Step-360,Loss-2.9818947315216064\n",
      "Step-361,Loss-3.4516284465789795\n",
      "Step-362,Loss-3.2046573162078857\n",
      "Step-363,Loss-3.0194077491760254\n",
      "Step-364,Loss-3.1538891792297363\n",
      "Step-365,Loss-3.14789080619812\n",
      "Step-366,Loss-2.653444290161133\n",
      "Step-367,Loss-3.18886399269104\n",
      "Step-368,Loss-2.881598949432373\n",
      "Step-369,Loss-2.3238093852996826\n",
      "Step-370,Loss-2.502114772796631\n",
      "Step-371,Loss-3.2731821537017822\n",
      "Step-372,Loss-2.6491949558258057\n",
      "Step-373,Loss-2.972353935241699\n",
      "Step-374,Loss-2.816757917404175\n",
      "Step-375,Loss-3.071990728378296\n",
      "Step-376,Loss-2.967806816101074\n",
      "Step-377,Loss-2.51137375831604\n",
      "Step-378,Loss-2.2715818881988525\n",
      "Step-379,Loss-3.103100538253784\n",
      "Step-380,Loss-2.4797797203063965\n",
      "Step-381,Loss-3.1938400268554688\n",
      "Step-382,Loss-3.2655556201934814\n",
      "Step-383,Loss-2.6930124759674072\n",
      "Step-384,Loss-1.7204492092132568\n",
      "Step-385,Loss-1.6122350692749023\n",
      "Step-386,Loss-3.4058315753936768\n",
      "Step-387,Loss-2.9450647830963135\n",
      "Step-388,Loss-3.3256547451019287\n",
      "Step-389,Loss-3.148697853088379\n",
      "Step-390,Loss-2.831383228302002\n",
      "Step-391,Loss-2.9301772117614746\n",
      "Step-392,Loss-2.752647876739502\n",
      "Step-393,Loss-3.3184280395507812\n",
      "Step-394,Loss-1.9046435356140137\n",
      "Step-395,Loss-1.8198446035385132\n",
      "Step-396,Loss-2.5385138988494873\n",
      "Step-397,Loss-1.7390422821044922\n",
      "Step-398,Loss-1.9422868490219116\n",
      "Step-399,Loss-2.936077833175659\n",
      "Step-400,Loss-2.6877808570861816\n",
      "Model and Tokneizer are saved in GPT2-model-medium\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# Training loop\n",
    "scaler = GradScaler()\n",
    "model.train()\n",
    "num_epochs=1\n",
    "output_path = 'GPT2-model-medium'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "       \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # autocast\n",
    "        with autocast():\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # scale loss with stability of mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "       \n",
    "        print(\"Step-{},Loss-{}\".format(step, loss.item()))\n",
    "        if step == 400:\n",
    "\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "\n",
    "\n",
    "            model.save_pretrained(output_path)\n",
    "            tokenizer.save_pretrained(output_path)\n",
    "\n",
    "            print(f\"Model and Tokneizer are saved in {output_path}\")\n",
    "            break\n",
    "            \n",
    "\n",
    "        #loss.backward()\n",
    "        #optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6d09a-2633-4f1a-9802-701718ed970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2d4d927c-1957-4d12-9baa-e1dd92a5cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved = GPT2LMHeadModel.from_pretrained('GPT2-model-medium')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('GPT2-model-medium')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b7d208c9-8c38-4f3a-b5df-ac1097c7552e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved.to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "40e7ddd3-1682-41a6-a7bb-1e3f18f379d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"he next six weeks will be hard as cases continue to explode and government leadership remains nonexistent. I can’t control any of this,\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "output = model_saved.generate(input_ids, max_length=100, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9e4248d-90e1-4858-82b4-407dbedd4b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 0 : he next six weeks will be hard as cases continue to explode and government leadership remains nonexistent. I can’t control any of this, but I can’t help but feel that the world’s attention is on the United States.\n",
      "\n",
      "The United States is the only country in the world that has a president who has not been elected by the people. The United States is the only country in the world that has a president who has not been elected by the people.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, generated in enumerate(output):\n",
    "    text = tokenizer.decode(generated, skip_special_token=True)\n",
    "    print(f'Generated Text {i} : {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f9c40-0da7-4464-961d-23411856e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
